# Data paths
data_path: 'data/raw/data.csv'
labels_path: 'data/raw/labels.csv'
processed_data_path: 'data/processed/'  # Path to save processed data

# CSV header settings
data_has_header: False               # True if data CSV has headers, False otherwise
labels_has_header: False             # True if labels CSV has headers, False otherwise

# Custom column names
data_columns: []                     # List of column names for data CSV (optional)
labels_columns: ['task', 'subject', 'trial']  # Column names for labels CSV

# Preprocessing settings
missing_value_strategy: 'mean'
categorical_columns: []              # No categorical columns to encode in features
scaling_method: 'none'               # Options: 'standard', 'minmax', 'none'

# Target variable settings
target_variable: 'subject'              # Options: 'task', 'subject'
task_type: 'user_auth'               # New parameter: 'user_id', 'task_id', 'user_auth'

# Cross-validation settings
cross_validation:
  task_identification:
    method: 'leave_one_subject_out'
  user_identification:
    method: 'repeated_k_fold'
    n_splits: 3
    n_repeats: 5
  user_authentication:
    method: 'custom'  # Custom method

# Feature Engineering Settings
feature_engineering:
  feature_selection_method: 'none'   # Options: 'univariate', 'mutual_info', 'model', 'none'
  k_best: 10                               # Number of features to select
  dimensionality_reduction_method: 'none'   # Options: 'pca', 'none'
  n_components: 5                          # Number of components to keep

# Model settings
models:
  # - name: 'RandomForest'
  #   parameters:
  #     n_estimators: [50, 100, 150, 200]
  #     max_depth: [null, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
  # - name: 'SVM'
  #   parameters:
  #     C: [0.001, 0.1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
  #     kernel: ['linear', 'rbf', 'sigmoid']
  - name: 'LogisticRegression'
    parameters:
      # C: [0.001, 0.1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
      C: [10]
      penalty: ['l2']
      solver: ['lbfgs']
  # - name: 'KNN'
  #   parameters:
  #     n_neighbors: [1, 3, 5, 7, 9, 11, 13, 15]
  #     weights: ['uniform', 'distance']
  # - name: 'DecisionTree'
  #   parameters:
  #     max_depth: [null, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
  #     criterion: ['gini', 'entropy', 'log_loss']
  # - name: 'NaiveBayes'
  #   parameters:
  #     var_smoothing: [0.00000000001, 0.0000000001, 0.000000001, 0.00000001, 0.0000001]
  # - name: 'GradientBoosting'
  #   parameters:
  #     n_estimators: [50, 100, 150, 200]
  #     learning_rate: [0.1, 0.05, 0.01, 0.005, 0.0001]
  #     max_depth: [null, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
  # - name: 'XGBoost'
  #   parameters:
  #     n_estimators: [50, 100, 150, 200]
  #     learning_rate: [0.1, 0.05, 0.01, 0.005, 0.0001]
  #     max_depth: [null, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
  # - name: 'lightgbm'
  #   parameters:
  #     n_estimators: [50, 100, 150, 200]
  #     learning_rate: [0.1, 0.05, 0.01, 0.005, 0.0001]
  #     max_depth: [null, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

# Other settings
random_seed: 42
